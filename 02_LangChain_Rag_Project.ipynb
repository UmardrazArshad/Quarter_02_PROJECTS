{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UmardrazArshad/Quarter_02_PROJECTS/blob/main/02_LangChain_Rag_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b50vFe1fd8eA",
        "outputId": "d317344d-c4bf-44ca-9204-abb861494dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Initialize Pinecone**"
      ],
      "metadata": {
        "id": "Ii_22XxU3d0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1gNVi_s9gAx-"
      },
      "outputs": [],
      "source": [
        "#  Initialize Pinecone\n",
        "from google.colab import userdata\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "6txZt6BchsC0"
      },
      "outputs": [],
      "source": [
        "#create index\n",
        "index_name = \"lanchain-rag-project-new\"  # change if desired\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-AjoOzBekxT0"
      },
      "outputs": [],
      "source": [
        "#Embedding\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "GE9-InhMli6L"
      },
      "outputs": [],
      "source": [
        "# Vectors\n",
        "vector = embeddings.embed_query(\" Tell me the road map ot learn Agentic Ai.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A_AgVOgl1d-",
        "outputId": "1e7a637b-77af-4a0d-8108-caae699e91a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.018941514194011688,\n",
              " -0.0410316176712513,\n",
              " 0.002591670723631978,\n",
              " -0.023518061265349388,\n",
              " 0.03384358808398247,\n",
              " 0.0043175071477890015]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "vector[:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QfPS4rKJmRjT"
      },
      "outputs": [],
      "source": [
        "# Vector Store\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set Up Document Loader**"
      ],
      "metadata": {
        "id": "N_lgOMvu3VF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt -q\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "\n",
        "# Instead of using TextLoader with an invalid encoding,\n",
        "# use Docx2txtLoader which is designed for .docx files.\n",
        "loader = Docx2txtLoader(\"/content/Agentic_AI_Expert_Roadmap.docx\")\n",
        "\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "BzXk5ukuz6pj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary class\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "XTeRJEsN5OJd"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embeddings**"
      ],
      "metadata": {
        "id": "ha5qJVttjjkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Create embeddings and upload to Pinecone\n",
        "for doc in tqdm(docs):\n",
        "    vector = embeddings.embed_query(doc.page_content)\n",
        "    # Change this line to provide a metadata dictionary\n",
        "    index.upsert([(doc.metadata[\"source\"], vector, {\"text\": doc.page_content})]) #  Add metadata as a dictionary with key 'text'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdv-p7aLh43A",
        "outputId": "01334af5-7efe-43fa-b7f7-3b111a6e7154"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:04<00:00,  2.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZfiVFOCF0CY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Set Up Retriever**"
      ],
      "metadata": {
        "id": "hkSFDTiYlLXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "# Use 'index' instead of 'pinecone_index'\n",
        "# Initialize PineconeVectorStore with the index and embeddings\n",
        "retriever = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "kcPZ1uTMjRse"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Raodmap to achieve agentic ai\"\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "id": "cmOn8ZRXlefZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614f4a36-c1f4-4e70-8ee6-776e2857300c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.556472] 5. **Freelancing Skills**: Effective client communication, proposal writing, and project delivery. [{}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set Up Google Gemini Flash Model**"
      ],
      "metadata": {
        "id": "wOoACjpz4FAp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "byQVXYOtQM5R"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Retriever and LLM\n",
        "from re import search\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embeddings\n",
        "    )\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",  # Other options: \"map_reduce\", \"refine\"\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "9Vjyu7YxwknS"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "6Ld5qvlNuWH7"
      },
      "outputs": [],
      "source": [
        "query = \"Give me the road map to learn&acheive Agentic AI\"\n",
        "def answer_to_user (query : str):\n",
        "\n",
        "  # Vector Search\n",
        "  vector_results = vector_store.similarity_search(query, k=2)\n",
        "  print (len(vector_results))\n",
        "\n",
        "  #pass to Model vector Results + User Query\n",
        "  final_answer = llm.invoke(f\"ANSWER THIS UAER QUERY : {query}, Here are some referance to answer {results}\")\n",
        "\n",
        "  return final_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Query the RAG System**"
      ],
      "metadata": {
        "id": "zkNAfThA4W2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "kLC0cf5MRdtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f2f7cc-b0c7-47cb-a235-04d71eb8b37a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "query = \"What kind of steps should i follow to learn agentic ai?\"\n",
        "\n",
        "# Calling answer_to_user with the 'query' string instead of the undefined 'final_answer'\n",
        "answer = answer_to_user(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "GbOtH8W6SVnL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "6293b3cb-6a6f-4a78-f597-742c5255f001"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'QUESTION : What kind of steps should i follow to learn agentic ai?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'RESPONSE (Generated by Gemini): content=\"The provided document fragment only mentions freelancing skills relevant to *working with* Agentic AI, not learning about it.  Therefore, I cannot answer your query using only that reference.  To learn about Agentic AI, you\\'ll need to follow a broader learning path.  This would likely involve:\\\\n\\\\n1. **Understanding Foundational Concepts:**  Start with a strong grasp of core AI concepts like machine learning, deep learning, reinforcement learning, and natural language processing (NLP).  Online courses (Coursera, edX, Udacity), textbooks, and university-level courses are excellent resources.\\\\n\\\\n2. **Focusing on Reinforcement Learning (RL):** Agentic AI heavily relies on RL, which allows agents to learn through trial and error in an environment.  Deep dive into RL algorithms like Q-learning, SARSA, and deep reinforcement learning (DRL) techniques.\\\\n\\\\n3. **Exploring Agent Architectures:**  Learn about different agent architectures, such as those based on Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs).\\\\n\\\\n4. **Studying Planning and Decision-Making:**  Understand how agents plan their actions and make decisions in complex environments.  This involves topics like search algorithms, game theory, and decision trees.\\\\n\\\\n5. **Investigating Specific Agentic AI Applications:**  Explore real-world applications of Agentic AI, such as robotics, game playing, autonomous driving, and resource management.  This will give you a practical understanding of how the concepts are applied.\\\\n\\\\n6. **Hands-on Practice:**  The most crucial step is to build your own agents.  Start with simple environments and gradually increase complexity.  Utilize libraries and frameworks like TensorFlow, PyTorch, and OpenAI Gym.\\\\n\\\\n7. **Staying Updated:**  The field of AI is rapidly evolving.  Stay current with the latest research papers, conferences, and online communities dedicated to AI and RL.\\\\n\\\\n\\\\nIn short, learning Agentic AI requires a significant commitment to learning core AI concepts and then specializing in reinforcement learning and agent architectures.  The freelancing skills mentioned in the document are only relevant *after* you have mastered the technical aspects.\\\\n\" additional_kwargs={} response_metadata={\\'prompt_feedback\\': {\\'block_reason\\': 0, \\'safety_ratings\\': []}, \\'finish_reason\\': \\'STOP\\', \\'safety_ratings\\': []} id=\\'run-92890e11-32cf-4168-a872-9219de23789a-0\\' usage_metadata={\\'input_tokens\\': 84, \\'output_tokens\\': 445, \\'total_tokens\\': 529, \\'input_token_details\\': {\\'cache_read\\': 0}}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "display (f\"QUESTION : {query}\")\n",
        "display (f\"RESPONSE (Generated by Gemini): {answer}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOR5zCgsj0R26lWCkV93iC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}